{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Main -- RNN of Language Modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARPqiJULo3xm"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "\n",
        "\n",
        "*   Babandeep Singh\n",
        "*   Robin Beura\n",
        "*   Rahul Gera\n",
        "*   Sanghmitra\n",
        "*   Yash Patel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEH_RjPRo1j0"
      },
      "source": [
        "##Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5xGkLoudxNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "444e2642-034a-4b76-b8ff-25a7659c49cb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/Shared drives/IDS 576/Assignment 3')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eW2yDeOFN2Q"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torch.utils.data import ConcatDataset\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDLXv_qUoFxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6a3087-2a3a-4f81-b4aa-4f472fa3e85a"
      },
      "source": [
        "SEED = 4116\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "#torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy', \n",
        "                  lower =True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "full_data = list(train_data.text)+list(test_data.text)\n",
        "print(\"Number of Sentences: \"+str(len(full_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences: 3890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmwWM6Sa4HEr"
      },
      "source": [
        "with open('IMDB_text.pkl', 'rb') as f:\n",
        "  full_data = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovkIPejE4meX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb9411b-943c-4738-8d45-724810068cd4"
      },
      "source": [
        "full_data\n",
        "print(\"Number of Sentences: \"+str(len(full_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Sentences: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iku73y2mStBQ"
      },
      "source": [
        "## Q-1.1 Markov N-gram model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyrhydooadH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc7a6cd-2944-4b4b-fdf0-e495d0ec8c80"
      },
      "source": [
        "'''Function for cleaning list of words\n",
        "    @Parameters: List: sequence\n",
        "    @Return: List:clean List (removing punctuations and numeric values)\n",
        "'''\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r'\\s+','',phrase)\n",
        "    #phrase = re.sub(r'<.*?>','', phrase)\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def clean_text(sentence):\n",
        "  decontraction = \" \".join([decontracted(word.strip()) for word in sentence])\n",
        "  clean_regex = re.compile('<.*?>')\n",
        "  word = re.sub(clean_regex,' ', decontraction)\n",
        "  pattern = r\"[{}]\".format(string.punctuation)\n",
        "  pharse = re.sub(pattern,' ',word)\n",
        "  clean_text = [word.lower() for word in pharse.split() if word]\n",
        "  return clean_text\n",
        "  \n",
        "print('Before: \\n'+' '.join(full_data[0]))\n",
        "print('\\nAfter: \\n'+' '.join(clean_text(full_data[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: \n",
            "Gundam Wing is a fun show . I appreciate it for getting me into Gundam and anime in general . However , after watching its predecessors , such as Mobile Suit Gundam , Zeta Gundam , and even G Gundam , I find Wing to be Gundam Lite.<br /><br />Characters : An aspect long held by Gundam is to have their characters thrust into difficulties and grow into maturity . This does not happen in Wing . Heero is top dog at the beginning , and he 's top dog at the end . Personalities do not change , growth is never achieved . The best character is Zechs , who is for all intents and purposes a hero throughout most of the series . But suddenly the series betrays him and turns him into a villain for no apparent reason.<br /><br />Mecha : Wing has great suit designs . The Gundams are super cool , with the Epyon being my favorite . I even consider a few of the OZ suit designs to be on par with some of the classic Zeon suits . But sweet suit designs does n't quite save the series from boring characters.<br /><br />Conclusion : In the end , Wing has cool fight scenes , though riddled with recycled animation , but shallow plot and character development . Enjoyable , but not moving like previous Gundam outings .\n",
            "\n",
            "After: \n",
            "gundam wing is a fun show i appreciate it for getting me into gundam and anime in general however after watching its predecessors such as mobile suit gundam zeta gundam and even g gundam i find wing to be gundam lite characters an aspect long held by gundam is to have their characters thrust into difficulties and grow into maturity this does not happen in wing heero is top dog at the beginning and he is top dog at the end personalities do not change growth is never achieved the best character is zechs who is for all intents and purposes a hero throughout most of the series but suddenly the series betrays him and turns him into a villain for no apparent reason mecha wing has great suit designs the gundams are super cool with the epyon being my favorite i even consider a few of the oz suit designs to be on par with some of the classic zeon suits but sweet suit designs does not quite save the series from boring characters conclusion in the end wing has cool fight scenes though riddled with recycled animation but shallow plot and character development enjoyable but not moving like previous gundam outings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijzN2_LGROm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf2600d-d26a-4f1c-d7d9-a59668364e45"
      },
      "source": [
        "''' Function for creating n-grams.\n",
        "    @Parameters: List: sequence, Integer: n (type of the n-gram).\n",
        "    @Return: List: n-grams.\n",
        "'''\n",
        "def generateNGrams(sequence, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(sequence)-n+1):\n",
        "        ngrams.append(tuple(sequence[i:i+n]))\n",
        "    #print(str(n)+\"-Grams Generated\\n\")\n",
        "    #print(ngrams[:5])\n",
        "    return ngrams\n",
        "\n",
        "example = generateNGrams(full_data[0],3)\n",
        "print(\"3-Grams Generated\\n\",example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3-Grams Generated\n",
            " [('Gundam', 'Wing', 'is'), ('Wing', 'is', 'a'), ('is', 'a', 'fun'), ('a', 'fun', 'show'), ('fun', 'show', '.'), ('show', '.', 'I'), ('.', 'I', 'appreciate'), ('I', 'appreciate', 'it'), ('appreciate', 'it', 'for'), ('it', 'for', 'getting'), ('for', 'getting', 'me'), ('getting', 'me', 'into'), ('me', 'into', 'Gundam'), ('into', 'Gundam', 'and'), ('Gundam', 'and', 'anime'), ('and', 'anime', 'in'), ('anime', 'in', 'general'), ('in', 'general', '.'), ('general', '.', 'However'), ('.', 'However', ','), ('However', ',', 'after'), (',', 'after', 'watching'), ('after', 'watching', 'its'), ('watching', 'its', 'predecessors'), ('its', 'predecessors', ','), ('predecessors', ',', 'such'), (',', 'such', 'as'), ('such', 'as', 'Mobile'), ('as', 'Mobile', 'Suit'), ('Mobile', 'Suit', 'Gundam'), ('Suit', 'Gundam', ','), ('Gundam', ',', 'Zeta'), (',', 'Zeta', 'Gundam'), ('Zeta', 'Gundam', ','), ('Gundam', ',', 'and'), (',', 'and', 'even'), ('and', 'even', 'G'), ('even', 'G', 'Gundam'), ('G', 'Gundam', ','), ('Gundam', ',', 'I'), (',', 'I', 'find'), ('I', 'find', 'Wing'), ('find', 'Wing', 'to'), ('Wing', 'to', 'be'), ('to', 'be', 'Gundam'), ('be', 'Gundam', 'Lite.<br'), ('Gundam', 'Lite.<br', '/><br'), ('Lite.<br', '/><br', '/>Characters'), ('/><br', '/>Characters', ':'), ('/>Characters', ':', 'An'), (':', 'An', 'aspect'), ('An', 'aspect', 'long'), ('aspect', 'long', 'held'), ('long', 'held', 'by'), ('held', 'by', 'Gundam'), ('by', 'Gundam', 'is'), ('Gundam', 'is', 'to'), ('is', 'to', 'have'), ('to', 'have', 'their'), ('have', 'their', 'characters'), ('their', 'characters', 'thrust'), ('characters', 'thrust', 'into'), ('thrust', 'into', 'difficulties'), ('into', 'difficulties', 'and'), ('difficulties', 'and', 'grow'), ('and', 'grow', 'into'), ('grow', 'into', 'maturity'), ('into', 'maturity', '.'), ('maturity', '.', 'This'), ('.', 'This', 'does'), ('This', 'does', 'not'), ('does', 'not', 'happen'), ('not', 'happen', 'in'), ('happen', 'in', 'Wing'), ('in', 'Wing', '.'), ('Wing', '.', 'Heero'), ('.', 'Heero', 'is'), ('Heero', 'is', 'top'), ('is', 'top', 'dog'), ('top', 'dog', 'at'), ('dog', 'at', 'the'), ('at', 'the', 'beginning'), ('the', 'beginning', ','), ('beginning', ',', 'and'), (',', 'and', 'he'), ('and', 'he', \"'s\"), ('he', \"'s\", 'top'), (\"'s\", 'top', 'dog'), ('top', 'dog', 'at'), ('dog', 'at', 'the'), ('at', 'the', 'end'), ('the', 'end', '.'), ('end', '.', 'Personalities'), ('.', 'Personalities', 'do'), ('Personalities', 'do', 'not'), ('do', 'not', 'change'), ('not', 'change', ','), ('change', ',', 'growth'), (',', 'growth', 'is'), ('growth', 'is', 'never'), ('is', 'never', 'achieved'), ('never', 'achieved', '.'), ('achieved', '.', 'The'), ('.', 'The', 'best'), ('The', 'best', 'character'), ('best', 'character', 'is'), ('character', 'is', 'Zechs'), ('is', 'Zechs', ','), ('Zechs', ',', 'who'), (',', 'who', 'is'), ('who', 'is', 'for'), ('is', 'for', 'all'), ('for', 'all', 'intents'), ('all', 'intents', 'and'), ('intents', 'and', 'purposes'), ('and', 'purposes', 'a'), ('purposes', 'a', 'hero'), ('a', 'hero', 'throughout'), ('hero', 'throughout', 'most'), ('throughout', 'most', 'of'), ('most', 'of', 'the'), ('of', 'the', 'series'), ('the', 'series', '.'), ('series', '.', 'But'), ('.', 'But', 'suddenly'), ('But', 'suddenly', 'the'), ('suddenly', 'the', 'series'), ('the', 'series', 'betrays'), ('series', 'betrays', 'him'), ('betrays', 'him', 'and'), ('him', 'and', 'turns'), ('and', 'turns', 'him'), ('turns', 'him', 'into'), ('him', 'into', 'a'), ('into', 'a', 'villain'), ('a', 'villain', 'for'), ('villain', 'for', 'no'), ('for', 'no', 'apparent'), ('no', 'apparent', 'reason.<br'), ('apparent', 'reason.<br', '/><br'), ('reason.<br', '/><br', '/>Mecha'), ('/><br', '/>Mecha', ':'), ('/>Mecha', ':', 'Wing'), (':', 'Wing', 'has'), ('Wing', 'has', 'great'), ('has', 'great', 'suit'), ('great', 'suit', 'designs'), ('suit', 'designs', '.'), ('designs', '.', 'The'), ('.', 'The', 'Gundams'), ('The', 'Gundams', 'are'), ('Gundams', 'are', 'super'), ('are', 'super', 'cool'), ('super', 'cool', ','), ('cool', ',', 'with'), (',', 'with', 'the'), ('with', 'the', 'Epyon'), ('the', 'Epyon', 'being'), ('Epyon', 'being', 'my'), ('being', 'my', 'favorite'), ('my', 'favorite', '.'), ('favorite', '.', 'I'), ('.', 'I', 'even'), ('I', 'even', 'consider'), ('even', 'consider', 'a'), ('consider', 'a', 'few'), ('a', 'few', 'of'), ('few', 'of', 'the'), ('of', 'the', 'OZ'), ('the', 'OZ', 'suit'), ('OZ', 'suit', 'designs'), ('suit', 'designs', 'to'), ('designs', 'to', 'be'), ('to', 'be', 'on'), ('be', 'on', 'par'), ('on', 'par', 'with'), ('par', 'with', 'some'), ('with', 'some', 'of'), ('some', 'of', 'the'), ('of', 'the', 'classic'), ('the', 'classic', 'Zeon'), ('classic', 'Zeon', 'suits'), ('Zeon', 'suits', '.'), ('suits', '.', 'But'), ('.', 'But', 'sweet'), ('But', 'sweet', 'suit'), ('sweet', 'suit', 'designs'), ('suit', 'designs', 'does'), ('designs', 'does', \"n't\"), ('does', \"n't\", 'quite'), (\"n't\", 'quite', 'save'), ('quite', 'save', 'the'), ('save', 'the', 'series'), ('the', 'series', 'from'), ('series', 'from', 'boring'), ('from', 'boring', 'characters.<br'), ('boring', 'characters.<br', '/><br'), ('characters.<br', '/><br', '/>Conclusion'), ('/><br', '/>Conclusion', ':'), ('/>Conclusion', ':', 'In'), (':', 'In', 'the'), ('In', 'the', 'end'), ('the', 'end', ','), ('end', ',', 'Wing'), (',', 'Wing', 'has'), ('Wing', 'has', 'cool'), ('has', 'cool', 'fight'), ('cool', 'fight', 'scenes'), ('fight', 'scenes', ','), ('scenes', ',', 'though'), (',', 'though', 'riddled'), ('though', 'riddled', 'with'), ('riddled', 'with', 'recycled'), ('with', 'recycled', 'animation'), ('recycled', 'animation', ','), ('animation', ',', 'but'), (',', 'but', 'shallow'), ('but', 'shallow', 'plot'), ('shallow', 'plot', 'and'), ('plot', 'and', 'character'), ('and', 'character', 'development'), ('character', 'development', '.'), ('development', '.', 'Enjoyable'), ('.', 'Enjoyable', ','), ('Enjoyable', ',', 'but'), (',', 'but', 'not'), ('but', 'not', 'moving'), ('not', 'moving', 'like'), ('moving', 'like', 'previous'), ('like', 'previous', 'Gundam'), ('previous', 'Gundam', 'outings'), ('Gundam', 'outings', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X24jHk3YRhAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44dd3884-a13d-4ca2-b8a6-a84123d59bde"
      },
      "source": [
        "''' Function for counting how often each item occurs in a sequence.\n",
        "    @Parameters: Tuple: N-Grams, Integer: n (type of the n-gram).\n",
        "    @Return: Dictionary: token:frequency.\n",
        "'''\n",
        "def getNGramFrequencies(ngrams):\n",
        "    frequencies = {}\n",
        "    for token in ngrams:\n",
        "        if (tuple(token) in frequencies):\n",
        "            newFrequency = str(int(frequencies[tuple(token)]) + 1)\n",
        "            frequencies.update({tuple(token): newFrequency})\n",
        "        else: # Just in the first time a token is found\n",
        "            frequencies.update({tuple(token): '1'})\n",
        "    sorted_freq = {k: v for k, v in sorted(frequencies.items(), key=lambda item: item[1],reverse=True)}\n",
        "    return sorted_freq\n",
        "\n",
        "print(\"Frequencies:\\n\",getNGramFrequencies(example))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frequencies:\n",
            " {('top', 'dog', 'at'): '2', ('dog', 'at', 'the'): '2', ('Gundam', 'Wing', 'is'): '1', ('Wing', 'is', 'a'): '1', ('is', 'a', 'fun'): '1', ('a', 'fun', 'show'): '1', ('fun', 'show', '.'): '1', ('show', '.', 'I'): '1', ('.', 'I', 'appreciate'): '1', ('I', 'appreciate', 'it'): '1', ('appreciate', 'it', 'for'): '1', ('it', 'for', 'getting'): '1', ('for', 'getting', 'me'): '1', ('getting', 'me', 'into'): '1', ('me', 'into', 'Gundam'): '1', ('into', 'Gundam', 'and'): '1', ('Gundam', 'and', 'anime'): '1', ('and', 'anime', 'in'): '1', ('anime', 'in', 'general'): '1', ('in', 'general', '.'): '1', ('general', '.', 'However'): '1', ('.', 'However', ','): '1', ('However', ',', 'after'): '1', (',', 'after', 'watching'): '1', ('after', 'watching', 'its'): '1', ('watching', 'its', 'predecessors'): '1', ('its', 'predecessors', ','): '1', ('predecessors', ',', 'such'): '1', (',', 'such', 'as'): '1', ('such', 'as', 'Mobile'): '1', ('as', 'Mobile', 'Suit'): '1', ('Mobile', 'Suit', 'Gundam'): '1', ('Suit', 'Gundam', ','): '1', ('Gundam', ',', 'Zeta'): '1', (',', 'Zeta', 'Gundam'): '1', ('Zeta', 'Gundam', ','): '1', ('Gundam', ',', 'and'): '1', (',', 'and', 'even'): '1', ('and', 'even', 'G'): '1', ('even', 'G', 'Gundam'): '1', ('G', 'Gundam', ','): '1', ('Gundam', ',', 'I'): '1', (',', 'I', 'find'): '1', ('I', 'find', 'Wing'): '1', ('find', 'Wing', 'to'): '1', ('Wing', 'to', 'be'): '1', ('to', 'be', 'Gundam'): '1', ('be', 'Gundam', 'Lite.<br'): '1', ('Gundam', 'Lite.<br', '/><br'): '1', ('Lite.<br', '/><br', '/>Characters'): '1', ('/><br', '/>Characters', ':'): '1', ('/>Characters', ':', 'An'): '1', (':', 'An', 'aspect'): '1', ('An', 'aspect', 'long'): '1', ('aspect', 'long', 'held'): '1', ('long', 'held', 'by'): '1', ('held', 'by', 'Gundam'): '1', ('by', 'Gundam', 'is'): '1', ('Gundam', 'is', 'to'): '1', ('is', 'to', 'have'): '1', ('to', 'have', 'their'): '1', ('have', 'their', 'characters'): '1', ('their', 'characters', 'thrust'): '1', ('characters', 'thrust', 'into'): '1', ('thrust', 'into', 'difficulties'): '1', ('into', 'difficulties', 'and'): '1', ('difficulties', 'and', 'grow'): '1', ('and', 'grow', 'into'): '1', ('grow', 'into', 'maturity'): '1', ('into', 'maturity', '.'): '1', ('maturity', '.', 'This'): '1', ('.', 'This', 'does'): '1', ('This', 'does', 'not'): '1', ('does', 'not', 'happen'): '1', ('not', 'happen', 'in'): '1', ('happen', 'in', 'Wing'): '1', ('in', 'Wing', '.'): '1', ('Wing', '.', 'Heero'): '1', ('.', 'Heero', 'is'): '1', ('Heero', 'is', 'top'): '1', ('is', 'top', 'dog'): '1', ('at', 'the', 'beginning'): '1', ('the', 'beginning', ','): '1', ('beginning', ',', 'and'): '1', (',', 'and', 'he'): '1', ('and', 'he', \"'s\"): '1', ('he', \"'s\", 'top'): '1', (\"'s\", 'top', 'dog'): '1', ('at', 'the', 'end'): '1', ('the', 'end', '.'): '1', ('end', '.', 'Personalities'): '1', ('.', 'Personalities', 'do'): '1', ('Personalities', 'do', 'not'): '1', ('do', 'not', 'change'): '1', ('not', 'change', ','): '1', ('change', ',', 'growth'): '1', (',', 'growth', 'is'): '1', ('growth', 'is', 'never'): '1', ('is', 'never', 'achieved'): '1', ('never', 'achieved', '.'): '1', ('achieved', '.', 'The'): '1', ('.', 'The', 'best'): '1', ('The', 'best', 'character'): '1', ('best', 'character', 'is'): '1', ('character', 'is', 'Zechs'): '1', ('is', 'Zechs', ','): '1', ('Zechs', ',', 'who'): '1', (',', 'who', 'is'): '1', ('who', 'is', 'for'): '1', ('is', 'for', 'all'): '1', ('for', 'all', 'intents'): '1', ('all', 'intents', 'and'): '1', ('intents', 'and', 'purposes'): '1', ('and', 'purposes', 'a'): '1', ('purposes', 'a', 'hero'): '1', ('a', 'hero', 'throughout'): '1', ('hero', 'throughout', 'most'): '1', ('throughout', 'most', 'of'): '1', ('most', 'of', 'the'): '1', ('of', 'the', 'series'): '1', ('the', 'series', '.'): '1', ('series', '.', 'But'): '1', ('.', 'But', 'suddenly'): '1', ('But', 'suddenly', 'the'): '1', ('suddenly', 'the', 'series'): '1', ('the', 'series', 'betrays'): '1', ('series', 'betrays', 'him'): '1', ('betrays', 'him', 'and'): '1', ('him', 'and', 'turns'): '1', ('and', 'turns', 'him'): '1', ('turns', 'him', 'into'): '1', ('him', 'into', 'a'): '1', ('into', 'a', 'villain'): '1', ('a', 'villain', 'for'): '1', ('villain', 'for', 'no'): '1', ('for', 'no', 'apparent'): '1', ('no', 'apparent', 'reason.<br'): '1', ('apparent', 'reason.<br', '/><br'): '1', ('reason.<br', '/><br', '/>Mecha'): '1', ('/><br', '/>Mecha', ':'): '1', ('/>Mecha', ':', 'Wing'): '1', (':', 'Wing', 'has'): '1', ('Wing', 'has', 'great'): '1', ('has', 'great', 'suit'): '1', ('great', 'suit', 'designs'): '1', ('suit', 'designs', '.'): '1', ('designs', '.', 'The'): '1', ('.', 'The', 'Gundams'): '1', ('The', 'Gundams', 'are'): '1', ('Gundams', 'are', 'super'): '1', ('are', 'super', 'cool'): '1', ('super', 'cool', ','): '1', ('cool', ',', 'with'): '1', (',', 'with', 'the'): '1', ('with', 'the', 'Epyon'): '1', ('the', 'Epyon', 'being'): '1', ('Epyon', 'being', 'my'): '1', ('being', 'my', 'favorite'): '1', ('my', 'favorite', '.'): '1', ('favorite', '.', 'I'): '1', ('.', 'I', 'even'): '1', ('I', 'even', 'consider'): '1', ('even', 'consider', 'a'): '1', ('consider', 'a', 'few'): '1', ('a', 'few', 'of'): '1', ('few', 'of', 'the'): '1', ('of', 'the', 'OZ'): '1', ('the', 'OZ', 'suit'): '1', ('OZ', 'suit', 'designs'): '1', ('suit', 'designs', 'to'): '1', ('designs', 'to', 'be'): '1', ('to', 'be', 'on'): '1', ('be', 'on', 'par'): '1', ('on', 'par', 'with'): '1', ('par', 'with', 'some'): '1', ('with', 'some', 'of'): '1', ('some', 'of', 'the'): '1', ('of', 'the', 'classic'): '1', ('the', 'classic', 'Zeon'): '1', ('classic', 'Zeon', 'suits'): '1', ('Zeon', 'suits', '.'): '1', ('suits', '.', 'But'): '1', ('.', 'But', 'sweet'): '1', ('But', 'sweet', 'suit'): '1', ('sweet', 'suit', 'designs'): '1', ('suit', 'designs', 'does'): '1', ('designs', 'does', \"n't\"): '1', ('does', \"n't\", 'quite'): '1', (\"n't\", 'quite', 'save'): '1', ('quite', 'save', 'the'): '1', ('save', 'the', 'series'): '1', ('the', 'series', 'from'): '1', ('series', 'from', 'boring'): '1', ('from', 'boring', 'characters.<br'): '1', ('boring', 'characters.<br', '/><br'): '1', ('characters.<br', '/><br', '/>Conclusion'): '1', ('/><br', '/>Conclusion', ':'): '1', ('/>Conclusion', ':', 'In'): '1', (':', 'In', 'the'): '1', ('In', 'the', 'end'): '1', ('the', 'end', ','): '1', ('end', ',', 'Wing'): '1', (',', 'Wing', 'has'): '1', ('Wing', 'has', 'cool'): '1', ('has', 'cool', 'fight'): '1', ('cool', 'fight', 'scenes'): '1', ('fight', 'scenes', ','): '1', ('scenes', ',', 'though'): '1', (',', 'though', 'riddled'): '1', ('though', 'riddled', 'with'): '1', ('riddled', 'with', 'recycled'): '1', ('with', 'recycled', 'animation'): '1', ('recycled', 'animation', ','): '1', ('animation', ',', 'but'): '1', (',', 'but', 'shallow'): '1', ('but', 'shallow', 'plot'): '1', ('shallow', 'plot', 'and'): '1', ('plot', 'and', 'character'): '1', ('and', 'character', 'development'): '1', ('character', 'development', '.'): '1', ('development', '.', 'Enjoyable'): '1', ('.', 'Enjoyable', ','): '1', ('Enjoyable', ',', 'but'): '1', (',', 'but', 'not'): '1', ('but', 'not', 'moving'): '1', ('not', 'moving', 'like'): '1', ('moving', 'like', 'previous'): '1', ('like', 'previous', 'Gundam'): '1', ('previous', 'Gundam', 'outings'): '1', ('Gundam', 'outings', '.'): '1'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDLZL7OkT1UT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0baf55-6492-4234-8223-8e3219d39737"
      },
      "source": [
        "''' Function for adding an occurance of a gram considering the one which is\n",
        "    following it.\n",
        "    probabilities: {\n",
        "        word_0: {\n",
        "            next_0: occurance_0, ...\n",
        "            next_n: occurance_n\n",
        "        }, ...\n",
        "        word_n: {\n",
        "            next_0: occurance_0, ...\n",
        "            next_n: occurance_n\n",
        "        },\n",
        "    }\n",
        "    @Parameters: String: currentWord, nextWord\n",
        "                 Dictionary: occurances.\n",
        "    @Return: Dictionary: new occurances.\n",
        "'''\n",
        "def addOccurance(currentWord, nextWord, occurances):\n",
        "    occurs = occurances\n",
        "    if (currentWord in occurs):\n",
        "        if (nextWord in occurs[currentWord]):\n",
        "            aux = int(occurs[currentWord][nextWord])\n",
        "            aux += 1\n",
        "            occurs[currentWord][nextWord] = str(aux)\n",
        "        else:\n",
        "            occurs[currentWord].update({nextWord : \"1\"})\n",
        "    else:\n",
        "        occurs[currentWord] = {nextWord : \"1\"}\n",
        "    return occurs\n",
        "\n",
        "''' Function for getting occurances of n-grams (word by the previous one).\n",
        "    @Parameters: Tuple: N-Grams.\n",
        "    @Return: Dictionary: occurances.\n",
        "'''\n",
        "def getNGramOccurances(ngrams):\n",
        "    occurs = {}\n",
        "    currentWord = 0\n",
        "    nextWord = 1\n",
        "    for i in range(len(ngrams) - 1):\n",
        "        occurs = addOccurance(ngrams[currentWord], ngrams[nextWord], occurs)\n",
        "        currentWord += 1\n",
        "        nextWord += 1\n",
        "    return occurs\n",
        "\n",
        "occur = getNGramOccurances(example)\n",
        "print(\"Possible Occurances after sequence of words\\n\",occur)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Possible Occurances after sequence of words\n",
            " {('Gundam', 'Wing', 'is'): {('Wing', 'is', 'a'): '1'}, ('Wing', 'is', 'a'): {('is', 'a', 'fun'): '1'}, ('is', 'a', 'fun'): {('a', 'fun', 'show'): '1'}, ('a', 'fun', 'show'): {('fun', 'show', '.'): '1'}, ('fun', 'show', '.'): {('show', '.', 'I'): '1'}, ('show', '.', 'I'): {('.', 'I', 'appreciate'): '1'}, ('.', 'I', 'appreciate'): {('I', 'appreciate', 'it'): '1'}, ('I', 'appreciate', 'it'): {('appreciate', 'it', 'for'): '1'}, ('appreciate', 'it', 'for'): {('it', 'for', 'getting'): '1'}, ('it', 'for', 'getting'): {('for', 'getting', 'me'): '1'}, ('for', 'getting', 'me'): {('getting', 'me', 'into'): '1'}, ('getting', 'me', 'into'): {('me', 'into', 'Gundam'): '1'}, ('me', 'into', 'Gundam'): {('into', 'Gundam', 'and'): '1'}, ('into', 'Gundam', 'and'): {('Gundam', 'and', 'anime'): '1'}, ('Gundam', 'and', 'anime'): {('and', 'anime', 'in'): '1'}, ('and', 'anime', 'in'): {('anime', 'in', 'general'): '1'}, ('anime', 'in', 'general'): {('in', 'general', '.'): '1'}, ('in', 'general', '.'): {('general', '.', 'However'): '1'}, ('general', '.', 'However'): {('.', 'However', ','): '1'}, ('.', 'However', ','): {('However', ',', 'after'): '1'}, ('However', ',', 'after'): {(',', 'after', 'watching'): '1'}, (',', 'after', 'watching'): {('after', 'watching', 'its'): '1'}, ('after', 'watching', 'its'): {('watching', 'its', 'predecessors'): '1'}, ('watching', 'its', 'predecessors'): {('its', 'predecessors', ','): '1'}, ('its', 'predecessors', ','): {('predecessors', ',', 'such'): '1'}, ('predecessors', ',', 'such'): {(',', 'such', 'as'): '1'}, (',', 'such', 'as'): {('such', 'as', 'Mobile'): '1'}, ('such', 'as', 'Mobile'): {('as', 'Mobile', 'Suit'): '1'}, ('as', 'Mobile', 'Suit'): {('Mobile', 'Suit', 'Gundam'): '1'}, ('Mobile', 'Suit', 'Gundam'): {('Suit', 'Gundam', ','): '1'}, ('Suit', 'Gundam', ','): {('Gundam', ',', 'Zeta'): '1'}, ('Gundam', ',', 'Zeta'): {(',', 'Zeta', 'Gundam'): '1'}, (',', 'Zeta', 'Gundam'): {('Zeta', 'Gundam', ','): '1'}, ('Zeta', 'Gundam', ','): {('Gundam', ',', 'and'): '1'}, ('Gundam', ',', 'and'): {(',', 'and', 'even'): '1'}, (',', 'and', 'even'): {('and', 'even', 'G'): '1'}, ('and', 'even', 'G'): {('even', 'G', 'Gundam'): '1'}, ('even', 'G', 'Gundam'): {('G', 'Gundam', ','): '1'}, ('G', 'Gundam', ','): {('Gundam', ',', 'I'): '1'}, ('Gundam', ',', 'I'): {(',', 'I', 'find'): '1'}, (',', 'I', 'find'): {('I', 'find', 'Wing'): '1'}, ('I', 'find', 'Wing'): {('find', 'Wing', 'to'): '1'}, ('find', 'Wing', 'to'): {('Wing', 'to', 'be'): '1'}, ('Wing', 'to', 'be'): {('to', 'be', 'Gundam'): '1'}, ('to', 'be', 'Gundam'): {('be', 'Gundam', 'Lite.<br'): '1'}, ('be', 'Gundam', 'Lite.<br'): {('Gundam', 'Lite.<br', '/><br'): '1'}, ('Gundam', 'Lite.<br', '/><br'): {('Lite.<br', '/><br', '/>Characters'): '1'}, ('Lite.<br', '/><br', '/>Characters'): {('/><br', '/>Characters', ':'): '1'}, ('/><br', '/>Characters', ':'): {('/>Characters', ':', 'An'): '1'}, ('/>Characters', ':', 'An'): {(':', 'An', 'aspect'): '1'}, (':', 'An', 'aspect'): {('An', 'aspect', 'long'): '1'}, ('An', 'aspect', 'long'): {('aspect', 'long', 'held'): '1'}, ('aspect', 'long', 'held'): {('long', 'held', 'by'): '1'}, ('long', 'held', 'by'): {('held', 'by', 'Gundam'): '1'}, ('held', 'by', 'Gundam'): {('by', 'Gundam', 'is'): '1'}, ('by', 'Gundam', 'is'): {('Gundam', 'is', 'to'): '1'}, ('Gundam', 'is', 'to'): {('is', 'to', 'have'): '1'}, ('is', 'to', 'have'): {('to', 'have', 'their'): '1'}, ('to', 'have', 'their'): {('have', 'their', 'characters'): '1'}, ('have', 'their', 'characters'): {('their', 'characters', 'thrust'): '1'}, ('their', 'characters', 'thrust'): {('characters', 'thrust', 'into'): '1'}, ('characters', 'thrust', 'into'): {('thrust', 'into', 'difficulties'): '1'}, ('thrust', 'into', 'difficulties'): {('into', 'difficulties', 'and'): '1'}, ('into', 'difficulties', 'and'): {('difficulties', 'and', 'grow'): '1'}, ('difficulties', 'and', 'grow'): {('and', 'grow', 'into'): '1'}, ('and', 'grow', 'into'): {('grow', 'into', 'maturity'): '1'}, ('grow', 'into', 'maturity'): {('into', 'maturity', '.'): '1'}, ('into', 'maturity', '.'): {('maturity', '.', 'This'): '1'}, ('maturity', '.', 'This'): {('.', 'This', 'does'): '1'}, ('.', 'This', 'does'): {('This', 'does', 'not'): '1'}, ('This', 'does', 'not'): {('does', 'not', 'happen'): '1'}, ('does', 'not', 'happen'): {('not', 'happen', 'in'): '1'}, ('not', 'happen', 'in'): {('happen', 'in', 'Wing'): '1'}, ('happen', 'in', 'Wing'): {('in', 'Wing', '.'): '1'}, ('in', 'Wing', '.'): {('Wing', '.', 'Heero'): '1'}, ('Wing', '.', 'Heero'): {('.', 'Heero', 'is'): '1'}, ('.', 'Heero', 'is'): {('Heero', 'is', 'top'): '1'}, ('Heero', 'is', 'top'): {('is', 'top', 'dog'): '1'}, ('is', 'top', 'dog'): {('top', 'dog', 'at'): '1'}, ('top', 'dog', 'at'): {('dog', 'at', 'the'): '2'}, ('dog', 'at', 'the'): {('at', 'the', 'beginning'): '1', ('at', 'the', 'end'): '1'}, ('at', 'the', 'beginning'): {('the', 'beginning', ','): '1'}, ('the', 'beginning', ','): {('beginning', ',', 'and'): '1'}, ('beginning', ',', 'and'): {(',', 'and', 'he'): '1'}, (',', 'and', 'he'): {('and', 'he', \"'s\"): '1'}, ('and', 'he', \"'s\"): {('he', \"'s\", 'top'): '1'}, ('he', \"'s\", 'top'): {(\"'s\", 'top', 'dog'): '1'}, (\"'s\", 'top', 'dog'): {('top', 'dog', 'at'): '1'}, ('at', 'the', 'end'): {('the', 'end', '.'): '1'}, ('the', 'end', '.'): {('end', '.', 'Personalities'): '1'}, ('end', '.', 'Personalities'): {('.', 'Personalities', 'do'): '1'}, ('.', 'Personalities', 'do'): {('Personalities', 'do', 'not'): '1'}, ('Personalities', 'do', 'not'): {('do', 'not', 'change'): '1'}, ('do', 'not', 'change'): {('not', 'change', ','): '1'}, ('not', 'change', ','): {('change', ',', 'growth'): '1'}, ('change', ',', 'growth'): {(',', 'growth', 'is'): '1'}, (',', 'growth', 'is'): {('growth', 'is', 'never'): '1'}, ('growth', 'is', 'never'): {('is', 'never', 'achieved'): '1'}, ('is', 'never', 'achieved'): {('never', 'achieved', '.'): '1'}, ('never', 'achieved', '.'): {('achieved', '.', 'The'): '1'}, ('achieved', '.', 'The'): {('.', 'The', 'best'): '1'}, ('.', 'The', 'best'): {('The', 'best', 'character'): '1'}, ('The', 'best', 'character'): {('best', 'character', 'is'): '1'}, ('best', 'character', 'is'): {('character', 'is', 'Zechs'): '1'}, ('character', 'is', 'Zechs'): {('is', 'Zechs', ','): '1'}, ('is', 'Zechs', ','): {('Zechs', ',', 'who'): '1'}, ('Zechs', ',', 'who'): {(',', 'who', 'is'): '1'}, (',', 'who', 'is'): {('who', 'is', 'for'): '1'}, ('who', 'is', 'for'): {('is', 'for', 'all'): '1'}, ('is', 'for', 'all'): {('for', 'all', 'intents'): '1'}, ('for', 'all', 'intents'): {('all', 'intents', 'and'): '1'}, ('all', 'intents', 'and'): {('intents', 'and', 'purposes'): '1'}, ('intents', 'and', 'purposes'): {('and', 'purposes', 'a'): '1'}, ('and', 'purposes', 'a'): {('purposes', 'a', 'hero'): '1'}, ('purposes', 'a', 'hero'): {('a', 'hero', 'throughout'): '1'}, ('a', 'hero', 'throughout'): {('hero', 'throughout', 'most'): '1'}, ('hero', 'throughout', 'most'): {('throughout', 'most', 'of'): '1'}, ('throughout', 'most', 'of'): {('most', 'of', 'the'): '1'}, ('most', 'of', 'the'): {('of', 'the', 'series'): '1'}, ('of', 'the', 'series'): {('the', 'series', '.'): '1'}, ('the', 'series', '.'): {('series', '.', 'But'): '1'}, ('series', '.', 'But'): {('.', 'But', 'suddenly'): '1'}, ('.', 'But', 'suddenly'): {('But', 'suddenly', 'the'): '1'}, ('But', 'suddenly', 'the'): {('suddenly', 'the', 'series'): '1'}, ('suddenly', 'the', 'series'): {('the', 'series', 'betrays'): '1'}, ('the', 'series', 'betrays'): {('series', 'betrays', 'him'): '1'}, ('series', 'betrays', 'him'): {('betrays', 'him', 'and'): '1'}, ('betrays', 'him', 'and'): {('him', 'and', 'turns'): '1'}, ('him', 'and', 'turns'): {('and', 'turns', 'him'): '1'}, ('and', 'turns', 'him'): {('turns', 'him', 'into'): '1'}, ('turns', 'him', 'into'): {('him', 'into', 'a'): '1'}, ('him', 'into', 'a'): {('into', 'a', 'villain'): '1'}, ('into', 'a', 'villain'): {('a', 'villain', 'for'): '1'}, ('a', 'villain', 'for'): {('villain', 'for', 'no'): '1'}, ('villain', 'for', 'no'): {('for', 'no', 'apparent'): '1'}, ('for', 'no', 'apparent'): {('no', 'apparent', 'reason.<br'): '1'}, ('no', 'apparent', 'reason.<br'): {('apparent', 'reason.<br', '/><br'): '1'}, ('apparent', 'reason.<br', '/><br'): {('reason.<br', '/><br', '/>Mecha'): '1'}, ('reason.<br', '/><br', '/>Mecha'): {('/><br', '/>Mecha', ':'): '1'}, ('/><br', '/>Mecha', ':'): {('/>Mecha', ':', 'Wing'): '1'}, ('/>Mecha', ':', 'Wing'): {(':', 'Wing', 'has'): '1'}, (':', 'Wing', 'has'): {('Wing', 'has', 'great'): '1'}, ('Wing', 'has', 'great'): {('has', 'great', 'suit'): '1'}, ('has', 'great', 'suit'): {('great', 'suit', 'designs'): '1'}, ('great', 'suit', 'designs'): {('suit', 'designs', '.'): '1'}, ('suit', 'designs', '.'): {('designs', '.', 'The'): '1'}, ('designs', '.', 'The'): {('.', 'The', 'Gundams'): '1'}, ('.', 'The', 'Gundams'): {('The', 'Gundams', 'are'): '1'}, ('The', 'Gundams', 'are'): {('Gundams', 'are', 'super'): '1'}, ('Gundams', 'are', 'super'): {('are', 'super', 'cool'): '1'}, ('are', 'super', 'cool'): {('super', 'cool', ','): '1'}, ('super', 'cool', ','): {('cool', ',', 'with'): '1'}, ('cool', ',', 'with'): {(',', 'with', 'the'): '1'}, (',', 'with', 'the'): {('with', 'the', 'Epyon'): '1'}, ('with', 'the', 'Epyon'): {('the', 'Epyon', 'being'): '1'}, ('the', 'Epyon', 'being'): {('Epyon', 'being', 'my'): '1'}, ('Epyon', 'being', 'my'): {('being', 'my', 'favorite'): '1'}, ('being', 'my', 'favorite'): {('my', 'favorite', '.'): '1'}, ('my', 'favorite', '.'): {('favorite', '.', 'I'): '1'}, ('favorite', '.', 'I'): {('.', 'I', 'even'): '1'}, ('.', 'I', 'even'): {('I', 'even', 'consider'): '1'}, ('I', 'even', 'consider'): {('even', 'consider', 'a'): '1'}, ('even', 'consider', 'a'): {('consider', 'a', 'few'): '1'}, ('consider', 'a', 'few'): {('a', 'few', 'of'): '1'}, ('a', 'few', 'of'): {('few', 'of', 'the'): '1'}, ('few', 'of', 'the'): {('of', 'the', 'OZ'): '1'}, ('of', 'the', 'OZ'): {('the', 'OZ', 'suit'): '1'}, ('the', 'OZ', 'suit'): {('OZ', 'suit', 'designs'): '1'}, ('OZ', 'suit', 'designs'): {('suit', 'designs', 'to'): '1'}, ('suit', 'designs', 'to'): {('designs', 'to', 'be'): '1'}, ('designs', 'to', 'be'): {('to', 'be', 'on'): '1'}, ('to', 'be', 'on'): {('be', 'on', 'par'): '1'}, ('be', 'on', 'par'): {('on', 'par', 'with'): '1'}, ('on', 'par', 'with'): {('par', 'with', 'some'): '1'}, ('par', 'with', 'some'): {('with', 'some', 'of'): '1'}, ('with', 'some', 'of'): {('some', 'of', 'the'): '1'}, ('some', 'of', 'the'): {('of', 'the', 'classic'): '1'}, ('of', 'the', 'classic'): {('the', 'classic', 'Zeon'): '1'}, ('the', 'classic', 'Zeon'): {('classic', 'Zeon', 'suits'): '1'}, ('classic', 'Zeon', 'suits'): {('Zeon', 'suits', '.'): '1'}, ('Zeon', 'suits', '.'): {('suits', '.', 'But'): '1'}, ('suits', '.', 'But'): {('.', 'But', 'sweet'): '1'}, ('.', 'But', 'sweet'): {('But', 'sweet', 'suit'): '1'}, ('But', 'sweet', 'suit'): {('sweet', 'suit', 'designs'): '1'}, ('sweet', 'suit', 'designs'): {('suit', 'designs', 'does'): '1'}, ('suit', 'designs', 'does'): {('designs', 'does', \"n't\"): '1'}, ('designs', 'does', \"n't\"): {('does', \"n't\", 'quite'): '1'}, ('does', \"n't\", 'quite'): {(\"n't\", 'quite', 'save'): '1'}, (\"n't\", 'quite', 'save'): {('quite', 'save', 'the'): '1'}, ('quite', 'save', 'the'): {('save', 'the', 'series'): '1'}, ('save', 'the', 'series'): {('the', 'series', 'from'): '1'}, ('the', 'series', 'from'): {('series', 'from', 'boring'): '1'}, ('series', 'from', 'boring'): {('from', 'boring', 'characters.<br'): '1'}, ('from', 'boring', 'characters.<br'): {('boring', 'characters.<br', '/><br'): '1'}, ('boring', 'characters.<br', '/><br'): {('characters.<br', '/><br', '/>Conclusion'): '1'}, ('characters.<br', '/><br', '/>Conclusion'): {('/><br', '/>Conclusion', ':'): '1'}, ('/><br', '/>Conclusion', ':'): {('/>Conclusion', ':', 'In'): '1'}, ('/>Conclusion', ':', 'In'): {(':', 'In', 'the'): '1'}, (':', 'In', 'the'): {('In', 'the', 'end'): '1'}, ('In', 'the', 'end'): {('the', 'end', ','): '1'}, ('the', 'end', ','): {('end', ',', 'Wing'): '1'}, ('end', ',', 'Wing'): {(',', 'Wing', 'has'): '1'}, (',', 'Wing', 'has'): {('Wing', 'has', 'cool'): '1'}, ('Wing', 'has', 'cool'): {('has', 'cool', 'fight'): '1'}, ('has', 'cool', 'fight'): {('cool', 'fight', 'scenes'): '1'}, ('cool', 'fight', 'scenes'): {('fight', 'scenes', ','): '1'}, ('fight', 'scenes', ','): {('scenes', ',', 'though'): '1'}, ('scenes', ',', 'though'): {(',', 'though', 'riddled'): '1'}, (',', 'though', 'riddled'): {('though', 'riddled', 'with'): '1'}, ('though', 'riddled', 'with'): {('riddled', 'with', 'recycled'): '1'}, ('riddled', 'with', 'recycled'): {('with', 'recycled', 'animation'): '1'}, ('with', 'recycled', 'animation'): {('recycled', 'animation', ','): '1'}, ('recycled', 'animation', ','): {('animation', ',', 'but'): '1'}, ('animation', ',', 'but'): {(',', 'but', 'shallow'): '1'}, (',', 'but', 'shallow'): {('but', 'shallow', 'plot'): '1'}, ('but', 'shallow', 'plot'): {('shallow', 'plot', 'and'): '1'}, ('shallow', 'plot', 'and'): {('plot', 'and', 'character'): '1'}, ('plot', 'and', 'character'): {('and', 'character', 'development'): '1'}, ('and', 'character', 'development'): {('character', 'development', '.'): '1'}, ('character', 'development', '.'): {('development', '.', 'Enjoyable'): '1'}, ('development', '.', 'Enjoyable'): {('.', 'Enjoyable', ','): '1'}, ('.', 'Enjoyable', ','): {('Enjoyable', ',', 'but'): '1'}, ('Enjoyable', ',', 'but'): {(',', 'but', 'not'): '1'}, (',', 'but', 'not'): {('but', 'not', 'moving'): '1'}, ('but', 'not', 'moving'): {('not', 'moving', 'like'): '1'}, ('not', 'moving', 'like'): {('moving', 'like', 'previous'): '1'}, ('moving', 'like', 'previous'): {('like', 'previous', 'Gundam'): '1'}, ('like', 'previous', 'Gundam'): {('previous', 'Gundam', 'outings'): '1'}, ('previous', 'Gundam', 'outings'): {('Gundam', 'outings', '.'): '1'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlFbFBqQRs5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed248ce-8d6a-46ae-83b3-84a8050b811b"
      },
      "source": [
        "''' Function for getting probabilities of n-grams (word by the previous one).\n",
        "    @Parameters: Dict: Occurances.\n",
        "    @Return: Dictionary: Probabilities.\n",
        "'''\n",
        "def getNGramProbabilities(occurances):\n",
        "    probs = {}\n",
        "    for pred in occurances:\n",
        "        auxNext = {}\n",
        "        for nextW in occurances[pred]:\n",
        "            auxNext[nextW] = str(int(occurances[pred][nextW]) / len(occurances[pred]))\n",
        "        probs[pred] = auxNext\n",
        "    return probs\n",
        "\n",
        "print(\"Probability of each Occurances after sequence of words\\n\",getNGramProbabilities(occur))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of each Occurances after sequence of words\n",
            " {('Gundam', 'Wing', 'is'): {('Wing', 'is', 'a'): '1.0'}, ('Wing', 'is', 'a'): {('is', 'a', 'fun'): '1.0'}, ('is', 'a', 'fun'): {('a', 'fun', 'show'): '1.0'}, ('a', 'fun', 'show'): {('fun', 'show', '.'): '1.0'}, ('fun', 'show', '.'): {('show', '.', 'I'): '1.0'}, ('show', '.', 'I'): {('.', 'I', 'appreciate'): '1.0'}, ('.', 'I', 'appreciate'): {('I', 'appreciate', 'it'): '1.0'}, ('I', 'appreciate', 'it'): {('appreciate', 'it', 'for'): '1.0'}, ('appreciate', 'it', 'for'): {('it', 'for', 'getting'): '1.0'}, ('it', 'for', 'getting'): {('for', 'getting', 'me'): '1.0'}, ('for', 'getting', 'me'): {('getting', 'me', 'into'): '1.0'}, ('getting', 'me', 'into'): {('me', 'into', 'Gundam'): '1.0'}, ('me', 'into', 'Gundam'): {('into', 'Gundam', 'and'): '1.0'}, ('into', 'Gundam', 'and'): {('Gundam', 'and', 'anime'): '1.0'}, ('Gundam', 'and', 'anime'): {('and', 'anime', 'in'): '1.0'}, ('and', 'anime', 'in'): {('anime', 'in', 'general'): '1.0'}, ('anime', 'in', 'general'): {('in', 'general', '.'): '1.0'}, ('in', 'general', '.'): {('general', '.', 'However'): '1.0'}, ('general', '.', 'However'): {('.', 'However', ','): '1.0'}, ('.', 'However', ','): {('However', ',', 'after'): '1.0'}, ('However', ',', 'after'): {(',', 'after', 'watching'): '1.0'}, (',', 'after', 'watching'): {('after', 'watching', 'its'): '1.0'}, ('after', 'watching', 'its'): {('watching', 'its', 'predecessors'): '1.0'}, ('watching', 'its', 'predecessors'): {('its', 'predecessors', ','): '1.0'}, ('its', 'predecessors', ','): {('predecessors', ',', 'such'): '1.0'}, ('predecessors', ',', 'such'): {(',', 'such', 'as'): '1.0'}, (',', 'such', 'as'): {('such', 'as', 'Mobile'): '1.0'}, ('such', 'as', 'Mobile'): {('as', 'Mobile', 'Suit'): '1.0'}, ('as', 'Mobile', 'Suit'): {('Mobile', 'Suit', 'Gundam'): '1.0'}, ('Mobile', 'Suit', 'Gundam'): {('Suit', 'Gundam', ','): '1.0'}, ('Suit', 'Gundam', ','): {('Gundam', ',', 'Zeta'): '1.0'}, ('Gundam', ',', 'Zeta'): {(',', 'Zeta', 'Gundam'): '1.0'}, (',', 'Zeta', 'Gundam'): {('Zeta', 'Gundam', ','): '1.0'}, ('Zeta', 'Gundam', ','): {('Gundam', ',', 'and'): '1.0'}, ('Gundam', ',', 'and'): {(',', 'and', 'even'): '1.0'}, (',', 'and', 'even'): {('and', 'even', 'G'): '1.0'}, ('and', 'even', 'G'): {('even', 'G', 'Gundam'): '1.0'}, ('even', 'G', 'Gundam'): {('G', 'Gundam', ','): '1.0'}, ('G', 'Gundam', ','): {('Gundam', ',', 'I'): '1.0'}, ('Gundam', ',', 'I'): {(',', 'I', 'find'): '1.0'}, (',', 'I', 'find'): {('I', 'find', 'Wing'): '1.0'}, ('I', 'find', 'Wing'): {('find', 'Wing', 'to'): '1.0'}, ('find', 'Wing', 'to'): {('Wing', 'to', 'be'): '1.0'}, ('Wing', 'to', 'be'): {('to', 'be', 'Gundam'): '1.0'}, ('to', 'be', 'Gundam'): {('be', 'Gundam', 'Lite.<br'): '1.0'}, ('be', 'Gundam', 'Lite.<br'): {('Gundam', 'Lite.<br', '/><br'): '1.0'}, ('Gundam', 'Lite.<br', '/><br'): {('Lite.<br', '/><br', '/>Characters'): '1.0'}, ('Lite.<br', '/><br', '/>Characters'): {('/><br', '/>Characters', ':'): '1.0'}, ('/><br', '/>Characters', ':'): {('/>Characters', ':', 'An'): '1.0'}, ('/>Characters', ':', 'An'): {(':', 'An', 'aspect'): '1.0'}, (':', 'An', 'aspect'): {('An', 'aspect', 'long'): '1.0'}, ('An', 'aspect', 'long'): {('aspect', 'long', 'held'): '1.0'}, ('aspect', 'long', 'held'): {('long', 'held', 'by'): '1.0'}, ('long', 'held', 'by'): {('held', 'by', 'Gundam'): '1.0'}, ('held', 'by', 'Gundam'): {('by', 'Gundam', 'is'): '1.0'}, ('by', 'Gundam', 'is'): {('Gundam', 'is', 'to'): '1.0'}, ('Gundam', 'is', 'to'): {('is', 'to', 'have'): '1.0'}, ('is', 'to', 'have'): {('to', 'have', 'their'): '1.0'}, ('to', 'have', 'their'): {('have', 'their', 'characters'): '1.0'}, ('have', 'their', 'characters'): {('their', 'characters', 'thrust'): '1.0'}, ('their', 'characters', 'thrust'): {('characters', 'thrust', 'into'): '1.0'}, ('characters', 'thrust', 'into'): {('thrust', 'into', 'difficulties'): '1.0'}, ('thrust', 'into', 'difficulties'): {('into', 'difficulties', 'and'): '1.0'}, ('into', 'difficulties', 'and'): {('difficulties', 'and', 'grow'): '1.0'}, ('difficulties', 'and', 'grow'): {('and', 'grow', 'into'): '1.0'}, ('and', 'grow', 'into'): {('grow', 'into', 'maturity'): '1.0'}, ('grow', 'into', 'maturity'): {('into', 'maturity', '.'): '1.0'}, ('into', 'maturity', '.'): {('maturity', '.', 'This'): '1.0'}, ('maturity', '.', 'This'): {('.', 'This', 'does'): '1.0'}, ('.', 'This', 'does'): {('This', 'does', 'not'): '1.0'}, ('This', 'does', 'not'): {('does', 'not', 'happen'): '1.0'}, ('does', 'not', 'happen'): {('not', 'happen', 'in'): '1.0'}, ('not', 'happen', 'in'): {('happen', 'in', 'Wing'): '1.0'}, ('happen', 'in', 'Wing'): {('in', 'Wing', '.'): '1.0'}, ('in', 'Wing', '.'): {('Wing', '.', 'Heero'): '1.0'}, ('Wing', '.', 'Heero'): {('.', 'Heero', 'is'): '1.0'}, ('.', 'Heero', 'is'): {('Heero', 'is', 'top'): '1.0'}, ('Heero', 'is', 'top'): {('is', 'top', 'dog'): '1.0'}, ('is', 'top', 'dog'): {('top', 'dog', 'at'): '1.0'}, ('top', 'dog', 'at'): {('dog', 'at', 'the'): '2.0'}, ('dog', 'at', 'the'): {('at', 'the', 'beginning'): '0.5', ('at', 'the', 'end'): '0.5'}, ('at', 'the', 'beginning'): {('the', 'beginning', ','): '1.0'}, ('the', 'beginning', ','): {('beginning', ',', 'and'): '1.0'}, ('beginning', ',', 'and'): {(',', 'and', 'he'): '1.0'}, (',', 'and', 'he'): {('and', 'he', \"'s\"): '1.0'}, ('and', 'he', \"'s\"): {('he', \"'s\", 'top'): '1.0'}, ('he', \"'s\", 'top'): {(\"'s\", 'top', 'dog'): '1.0'}, (\"'s\", 'top', 'dog'): {('top', 'dog', 'at'): '1.0'}, ('at', 'the', 'end'): {('the', 'end', '.'): '1.0'}, ('the', 'end', '.'): {('end', '.', 'Personalities'): '1.0'}, ('end', '.', 'Personalities'): {('.', 'Personalities', 'do'): '1.0'}, ('.', 'Personalities', 'do'): {('Personalities', 'do', 'not'): '1.0'}, ('Personalities', 'do', 'not'): {('do', 'not', 'change'): '1.0'}, ('do', 'not', 'change'): {('not', 'change', ','): '1.0'}, ('not', 'change', ','): {('change', ',', 'growth'): '1.0'}, ('change', ',', 'growth'): {(',', 'growth', 'is'): '1.0'}, (',', 'growth', 'is'): {('growth', 'is', 'never'): '1.0'}, ('growth', 'is', 'never'): {('is', 'never', 'achieved'): '1.0'}, ('is', 'never', 'achieved'): {('never', 'achieved', '.'): '1.0'}, ('never', 'achieved', '.'): {('achieved', '.', 'The'): '1.0'}, ('achieved', '.', 'The'): {('.', 'The', 'best'): '1.0'}, ('.', 'The', 'best'): {('The', 'best', 'character'): '1.0'}, ('The', 'best', 'character'): {('best', 'character', 'is'): '1.0'}, ('best', 'character', 'is'): {('character', 'is', 'Zechs'): '1.0'}, ('character', 'is', 'Zechs'): {('is', 'Zechs', ','): '1.0'}, ('is', 'Zechs', ','): {('Zechs', ',', 'who'): '1.0'}, ('Zechs', ',', 'who'): {(',', 'who', 'is'): '1.0'}, (',', 'who', 'is'): {('who', 'is', 'for'): '1.0'}, ('who', 'is', 'for'): {('is', 'for', 'all'): '1.0'}, ('is', 'for', 'all'): {('for', 'all', 'intents'): '1.0'}, ('for', 'all', 'intents'): {('all', 'intents', 'and'): '1.0'}, ('all', 'intents', 'and'): {('intents', 'and', 'purposes'): '1.0'}, ('intents', 'and', 'purposes'): {('and', 'purposes', 'a'): '1.0'}, ('and', 'purposes', 'a'): {('purposes', 'a', 'hero'): '1.0'}, ('purposes', 'a', 'hero'): {('a', 'hero', 'throughout'): '1.0'}, ('a', 'hero', 'throughout'): {('hero', 'throughout', 'most'): '1.0'}, ('hero', 'throughout', 'most'): {('throughout', 'most', 'of'): '1.0'}, ('throughout', 'most', 'of'): {('most', 'of', 'the'): '1.0'}, ('most', 'of', 'the'): {('of', 'the', 'series'): '1.0'}, ('of', 'the', 'series'): {('the', 'series', '.'): '1.0'}, ('the', 'series', '.'): {('series', '.', 'But'): '1.0'}, ('series', '.', 'But'): {('.', 'But', 'suddenly'): '1.0'}, ('.', 'But', 'suddenly'): {('But', 'suddenly', 'the'): '1.0'}, ('But', 'suddenly', 'the'): {('suddenly', 'the', 'series'): '1.0'}, ('suddenly', 'the', 'series'): {('the', 'series', 'betrays'): '1.0'}, ('the', 'series', 'betrays'): {('series', 'betrays', 'him'): '1.0'}, ('series', 'betrays', 'him'): {('betrays', 'him', 'and'): '1.0'}, ('betrays', 'him', 'and'): {('him', 'and', 'turns'): '1.0'}, ('him', 'and', 'turns'): {('and', 'turns', 'him'): '1.0'}, ('and', 'turns', 'him'): {('turns', 'him', 'into'): '1.0'}, ('turns', 'him', 'into'): {('him', 'into', 'a'): '1.0'}, ('him', 'into', 'a'): {('into', 'a', 'villain'): '1.0'}, ('into', 'a', 'villain'): {('a', 'villain', 'for'): '1.0'}, ('a', 'villain', 'for'): {('villain', 'for', 'no'): '1.0'}, ('villain', 'for', 'no'): {('for', 'no', 'apparent'): '1.0'}, ('for', 'no', 'apparent'): {('no', 'apparent', 'reason.<br'): '1.0'}, ('no', 'apparent', 'reason.<br'): {('apparent', 'reason.<br', '/><br'): '1.0'}, ('apparent', 'reason.<br', '/><br'): {('reason.<br', '/><br', '/>Mecha'): '1.0'}, ('reason.<br', '/><br', '/>Mecha'): {('/><br', '/>Mecha', ':'): '1.0'}, ('/><br', '/>Mecha', ':'): {('/>Mecha', ':', 'Wing'): '1.0'}, ('/>Mecha', ':', 'Wing'): {(':', 'Wing', 'has'): '1.0'}, (':', 'Wing', 'has'): {('Wing', 'has', 'great'): '1.0'}, ('Wing', 'has', 'great'): {('has', 'great', 'suit'): '1.0'}, ('has', 'great', 'suit'): {('great', 'suit', 'designs'): '1.0'}, ('great', 'suit', 'designs'): {('suit', 'designs', '.'): '1.0'}, ('suit', 'designs', '.'): {('designs', '.', 'The'): '1.0'}, ('designs', '.', 'The'): {('.', 'The', 'Gundams'): '1.0'}, ('.', 'The', 'Gundams'): {('The', 'Gundams', 'are'): '1.0'}, ('The', 'Gundams', 'are'): {('Gundams', 'are', 'super'): '1.0'}, ('Gundams', 'are', 'super'): {('are', 'super', 'cool'): '1.0'}, ('are', 'super', 'cool'): {('super', 'cool', ','): '1.0'}, ('super', 'cool', ','): {('cool', ',', 'with'): '1.0'}, ('cool', ',', 'with'): {(',', 'with', 'the'): '1.0'}, (',', 'with', 'the'): {('with', 'the', 'Epyon'): '1.0'}, ('with', 'the', 'Epyon'): {('the', 'Epyon', 'being'): '1.0'}, ('the', 'Epyon', 'being'): {('Epyon', 'being', 'my'): '1.0'}, ('Epyon', 'being', 'my'): {('being', 'my', 'favorite'): '1.0'}, ('being', 'my', 'favorite'): {('my', 'favorite', '.'): '1.0'}, ('my', 'favorite', '.'): {('favorite', '.', 'I'): '1.0'}, ('favorite', '.', 'I'): {('.', 'I', 'even'): '1.0'}, ('.', 'I', 'even'): {('I', 'even', 'consider'): '1.0'}, ('I', 'even', 'consider'): {('even', 'consider', 'a'): '1.0'}, ('even', 'consider', 'a'): {('consider', 'a', 'few'): '1.0'}, ('consider', 'a', 'few'): {('a', 'few', 'of'): '1.0'}, ('a', 'few', 'of'): {('few', 'of', 'the'): '1.0'}, ('few', 'of', 'the'): {('of', 'the', 'OZ'): '1.0'}, ('of', 'the', 'OZ'): {('the', 'OZ', 'suit'): '1.0'}, ('the', 'OZ', 'suit'): {('OZ', 'suit', 'designs'): '1.0'}, ('OZ', 'suit', 'designs'): {('suit', 'designs', 'to'): '1.0'}, ('suit', 'designs', 'to'): {('designs', 'to', 'be'): '1.0'}, ('designs', 'to', 'be'): {('to', 'be', 'on'): '1.0'}, ('to', 'be', 'on'): {('be', 'on', 'par'): '1.0'}, ('be', 'on', 'par'): {('on', 'par', 'with'): '1.0'}, ('on', 'par', 'with'): {('par', 'with', 'some'): '1.0'}, ('par', 'with', 'some'): {('with', 'some', 'of'): '1.0'}, ('with', 'some', 'of'): {('some', 'of', 'the'): '1.0'}, ('some', 'of', 'the'): {('of', 'the', 'classic'): '1.0'}, ('of', 'the', 'classic'): {('the', 'classic', 'Zeon'): '1.0'}, ('the', 'classic', 'Zeon'): {('classic', 'Zeon', 'suits'): '1.0'}, ('classic', 'Zeon', 'suits'): {('Zeon', 'suits', '.'): '1.0'}, ('Zeon', 'suits', '.'): {('suits', '.', 'But'): '1.0'}, ('suits', '.', 'But'): {('.', 'But', 'sweet'): '1.0'}, ('.', 'But', 'sweet'): {('But', 'sweet', 'suit'): '1.0'}, ('But', 'sweet', 'suit'): {('sweet', 'suit', 'designs'): '1.0'}, ('sweet', 'suit', 'designs'): {('suit', 'designs', 'does'): '1.0'}, ('suit', 'designs', 'does'): {('designs', 'does', \"n't\"): '1.0'}, ('designs', 'does', \"n't\"): {('does', \"n't\", 'quite'): '1.0'}, ('does', \"n't\", 'quite'): {(\"n't\", 'quite', 'save'): '1.0'}, (\"n't\", 'quite', 'save'): {('quite', 'save', 'the'): '1.0'}, ('quite', 'save', 'the'): {('save', 'the', 'series'): '1.0'}, ('save', 'the', 'series'): {('the', 'series', 'from'): '1.0'}, ('the', 'series', 'from'): {('series', 'from', 'boring'): '1.0'}, ('series', 'from', 'boring'): {('from', 'boring', 'characters.<br'): '1.0'}, ('from', 'boring', 'characters.<br'): {('boring', 'characters.<br', '/><br'): '1.0'}, ('boring', 'characters.<br', '/><br'): {('characters.<br', '/><br', '/>Conclusion'): '1.0'}, ('characters.<br', '/><br', '/>Conclusion'): {('/><br', '/>Conclusion', ':'): '1.0'}, ('/><br', '/>Conclusion', ':'): {('/>Conclusion', ':', 'In'): '1.0'}, ('/>Conclusion', ':', 'In'): {(':', 'In', 'the'): '1.0'}, (':', 'In', 'the'): {('In', 'the', 'end'): '1.0'}, ('In', 'the', 'end'): {('the', 'end', ','): '1.0'}, ('the', 'end', ','): {('end', ',', 'Wing'): '1.0'}, ('end', ',', 'Wing'): {(',', 'Wing', 'has'): '1.0'}, (',', 'Wing', 'has'): {('Wing', 'has', 'cool'): '1.0'}, ('Wing', 'has', 'cool'): {('has', 'cool', 'fight'): '1.0'}, ('has', 'cool', 'fight'): {('cool', 'fight', 'scenes'): '1.0'}, ('cool', 'fight', 'scenes'): {('fight', 'scenes', ','): '1.0'}, ('fight', 'scenes', ','): {('scenes', ',', 'though'): '1.0'}, ('scenes', ',', 'though'): {(',', 'though', 'riddled'): '1.0'}, (',', 'though', 'riddled'): {('though', 'riddled', 'with'): '1.0'}, ('though', 'riddled', 'with'): {('riddled', 'with', 'recycled'): '1.0'}, ('riddled', 'with', 'recycled'): {('with', 'recycled', 'animation'): '1.0'}, ('with', 'recycled', 'animation'): {('recycled', 'animation', ','): '1.0'}, ('recycled', 'animation', ','): {('animation', ',', 'but'): '1.0'}, ('animation', ',', 'but'): {(',', 'but', 'shallow'): '1.0'}, (',', 'but', 'shallow'): {('but', 'shallow', 'plot'): '1.0'}, ('but', 'shallow', 'plot'): {('shallow', 'plot', 'and'): '1.0'}, ('shallow', 'plot', 'and'): {('plot', 'and', 'character'): '1.0'}, ('plot', 'and', 'character'): {('and', 'character', 'development'): '1.0'}, ('and', 'character', 'development'): {('character', 'development', '.'): '1.0'}, ('character', 'development', '.'): {('development', '.', 'Enjoyable'): '1.0'}, ('development', '.', 'Enjoyable'): {('.', 'Enjoyable', ','): '1.0'}, ('.', 'Enjoyable', ','): {('Enjoyable', ',', 'but'): '1.0'}, ('Enjoyable', ',', 'but'): {(',', 'but', 'not'): '1.0'}, (',', 'but', 'not'): {('but', 'not', 'moving'): '1.0'}, ('but', 'not', 'moving'): {('not', 'moving', 'like'): '1.0'}, ('not', 'moving', 'like'): {('moving', 'like', 'previous'): '1.0'}, ('moving', 'like', 'previous'): {('like', 'previous', 'Gundam'): '1.0'}, ('like', 'previous', 'Gundam'): {('previous', 'Gundam', 'outings'): '1.0'}, ('previous', 'Gundam', 'outings'): {('Gundam', 'outings', '.'): '1.0'}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oSdXo-OGBwk"
      },
      "source": [
        "import time\n",
        "#Markov Model\n",
        "def markov_model(text,ngram):\n",
        "\n",
        "  start_time = time.time()\n",
        "  probs_dict = {}\n",
        "  ngrams_list = []\n",
        "  count = 0\n",
        "  for count,sentence in enumerate(text):\n",
        "    clean_sentence = clean_text(sentence)\n",
        "  #print('clean sentence')\n",
        "    ngrams = generateNGrams(clean_sentence,ngram)\n",
        "  #print('ngrams generate')\n",
        "    ngrams_list+=ngrams\n",
        "  #freq = getNGramFrequencies(ngrams_list)\n",
        "  #print('freq counted')\n",
        "\n",
        "  occurance = getNGramOccurances(ngrams_list)\n",
        "  #print('occurances got')\n",
        "  probs_dict.update(getNGramProbabilities(occurance))\n",
        "  \n",
        "  time_taken = round(time.time() - start_time,2)\n",
        "  #print(\"Time Taken to run--- %s seconds ---\" % (round(time.time() - start_time,2)))\n",
        "  return probs_dict,time_taken"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKhdNcJEHFEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6806e749-326b-4764-c441-dfc325f5bbde"
      },
      "source": [
        "prob_dict_ngrams = []\n",
        "time_taken = []\n",
        "\n",
        "\n",
        "def generate_2g(count, initial,prob_dict,rank):\n",
        "    if not count:\n",
        "        return initial[0] + ' ' + initial[1]\n",
        "    x = prob_dict.get(initial)\n",
        "    nxt = random.sample(list(x.keys()),1)[0]\n",
        "    #nxt = sorted(x.items(), key=operator.itemgetter(1),reverse=True)[rank][0]\n",
        "    return  str(initial[0]) + ' ' +generate_2g(count - 1, nxt,prob_dict,rank)\n",
        "\n",
        "def generate_3g(count, initial,prob_dict,rank):\n",
        "    if not count:\n",
        "        return initial[0] + ' ' + initial[1]\n",
        "    x = prob_dict.get(initial)\n",
        "    nxt = random.sample(list(x.keys()),1)[0]\n",
        "    #nxt = sorted(x.items(), key=operator.itemgetter(1),reverse=True)[rank][0]\n",
        "    return initial[0] + ' ' + generate_3g(count - 1, nxt,prob_dict,rank)\n",
        "\n",
        "\n",
        "model_2 = markov_model(full_data,2)\n",
        "print(str(2)+'-gram model trained!!\\n')\n",
        "for j in range(5):\n",
        "  print('-- my',generate_2g(20,('favourite','movie'),model_2[0],0))\n",
        "print(\"\\nTime Taken to run--- %s seconds ---\" % (model_2[1]))\n",
        "print(\"\\n\")\n",
        "\n",
        "model_3 = markov_model(full_data,3)\n",
        "print(str(3)+'-gram model trained!!\\n')\n",
        "for k in range(5):\n",
        "  print('--',generate_3g(20,('my','favourite','movie'),model_3[0],0))\n",
        "print(\"\\nTime Taken to run--- %s seconds ---\" % (model_3[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2-gram model trained!!\n",
            "\n",
            "-- my favourite movie wen i was pretty washboard the kid asks for trouble just yet instead the interesting interviews with authors philosophers and\n",
            "-- my favourite movie wen i mean origins really what can possibly happen in the largely unsung jewels of dialogue does not that hollywood\n",
            "-- my favourite movie i eagerly wait for him okay our story setup if you alienate the audience entrenched in ritual and learn toulon\n",
            "-- my favourite movie i rated films vaughn seems like plain suicide on just sheer fun so this deserves this film coasts to success\n",
            "-- my favourite movie my basic rule when living conditions with the irresistible bert lahr dressed in designer jeans but the grudge has a\n",
            "\n",
            "Time Taken to run--- 124.12 seconds ---\n",
            "\n",
            "\n",
            "3-gram model trained!!\n",
            "\n",
            "-- my favourite movie about jesus is ministry but that is forgivable because even though we might not notice  and on ad\n",
            "-- my favourite movie of all things sensory and how much coffee he drinks but he is head strong in stopping leary from\n",
            "-- my favourite movie since it did have moments of heart breaking honesty even while much of the problems though i suspect the\n",
            "-- my favourite movie wen i was 5 19 now and i just found out they both grew up in southern california for\n",
            "-- my favourite movie wen i was 5 at the 3rd annual roger ebert overlooked film festival after watching the emotional brokedown palace\n",
            "\n",
            "Time Taken to run--- 127.41 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzgqqVamOCd9"
      },
      "source": [
        "## Q1. 2 LSTM Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxZ_fzb7Pgec"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hru7DqEu15zX"
      },
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi_4rur7Kvid"
      },
      "source": [
        "def generateNGrams(sequence, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(sequence)-n+1):\n",
        "        ngrams.append(' '.join(tuple(sequence[i:i+n])))\n",
        "    #print(str(n)+\"-Grams Generated\\n\")\n",
        "    #print(ngrams[:5])\n",
        "    return ngrams\n",
        "    \n",
        "# making a list of words \n",
        "\"\"\" we have a nested list, making it into tokens\n",
        "\"\"\"\n",
        "from collections import Iterable\n",
        "def flatten(lis):\n",
        "     for item in lis:\n",
        "         if isinstance(item, Iterable) and not isinstance(item, str):\n",
        "             for x in flatten(item):\n",
        "                 yield x\n",
        "         else:        \n",
        "             yield item\n",
        "\n",
        "def joinStrings(text):\n",
        "    return ' '.join(string for string in text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9yrCvyl17S9"
      },
      "source": [
        "def prep_data(corpus_text):\n",
        "  word_counts = Counter(corpus_text)\n",
        "  sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "# Converting int to words \n",
        "  int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
        "\n",
        "# words to int \n",
        "  vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
        "  n_vocab = len(int_to_vocab)\n",
        "\n",
        "  print('Vocabulary size', n_vocab)\n",
        "\n",
        "  int_text = [vocab_to_int[w] for w in corpus_text]\n",
        "  num_batches = int(len(int_text) / (seq_size * batch_size))\n",
        "\n",
        "  in_text = int_text[:num_batches * batch_size * seq_size]\n",
        "\n",
        "  out_text = np.zeros_like(in_text)\n",
        "  out_text[:-1] = in_text[1:]\n",
        "  out_text[-1] = in_text[0]\n",
        "  in_text = np.reshape(in_text, (batch_size, -1))\n",
        "  out_text = np.reshape(out_text, (batch_size, -1))\n",
        "  return in_text,out_text,n_vocab,int_to_vocab,vocab_to_int\n",
        "\n",
        "def get_batches(in_text, out_text, batch_size, seq_size):\n",
        "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
        "    for i in range(0, num_batches * seq_size, seq_size):\n",
        "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7bnlQ2LyGj"
      },
      "source": [
        "class RNNModule(nn.Module):\n",
        "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
        "        super(RNNModule, self).__init__()\n",
        "        self.seq_size = seq_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size,\n",
        "                            lstm_size,\n",
        "                            batch_first=True)\n",
        "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.dense(output)\n",
        "\n",
        "        return logits, state\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
        "                torch.zeros(1, batch_size, self.lstm_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgNOxxUdMGdm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  loss_values= []\n",
        "  batches = get_batches(in_text, out_text, batch_size, seq_size)\n",
        "  state_h, state_c = model.zero_state(batch_size)\n",
        "\n",
        "  # Transfer data to GPU\n",
        "  state_h = state_h.to(device)\n",
        "  state_c = state_c.to(device)\n",
        "  for x, y in batches:\n",
        "    #iteration += 1\n",
        "    # Tell it we are in training mode\n",
        "    model.train()\n",
        "    # Reset all gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Transfer data to GPU\n",
        "    x = torch.tensor(x).to(device)\n",
        "    y = torch.tensor(y).to(device)\n",
        "    logits, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "    loss = criterion(logits.transpose(1, 2), y)\n",
        "    loss_value = loss.item()\n",
        "    loss.backward()\n",
        "    state_h = state_h.detach()\n",
        "    state_c = state_c.detach()\n",
        "    _ = torch.nn.utils.clip_grad_norm_(model.parameters(),5)\n",
        "    optimizer.step()\n",
        "    loss_values.append(loss_value)\n",
        "  return np.mean(loss_values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T04465sl2a-W"
      },
      "source": [
        "# LSTM Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boxQKFSGO6sM"
      },
      "source": [
        "###1-gram LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YJsIQu67VhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23089af6-fa01-4097-c57e-a93f3056dc08"
      },
      "source": [
        "import time\n",
        "seq_size=20\n",
        "batch_size=64\n",
        "embedding_size=64\n",
        "lstm_size=64\n",
        "initial_words='my favourite movie'.split()\n",
        "predict_top_k=5\n",
        "checkpoint_path='checkpoint'\n",
        "ngrams=1\n",
        "learning_rate = 0.1\n",
        "\n",
        "clean_full_data = [clean_text(rev) for rev in full_data[:1000]]\n",
        "text = joinStrings(list(flatten(clean_full_data)))\n",
        "corpus_text = generateNGrams(text.split(),ngrams)\n",
        "in_text,out_text,n_vocab, int_to_vocab,vocab_to_int= prep_data(corpus_text) # changed here \n",
        "batches = get_batches(in_text, out_text, batch_size, seq_size)\n",
        "\n",
        "n_epochs = 5\n",
        "sentences = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_onegram = RNNModule(n_vocab, seq_size,embedding_size, lstm_size)\n",
        "model_onegram = model_onegram.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model_onegram.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  start_time = time.time() \n",
        "  loss_values = train(model_onegram, batches, optimizer, criterion)\n",
        "  print('\\nEpoch: {}/{}'.format(e, n_epochs),\n",
        "        'Loss: {}'.format(np.mean(loss_values)))\n",
        "  time_taken = round(time.time() - start_time,2)\n",
        "  print(\"Time Taken to run epoch--- %s seconds ---\" % (round(time_taken,2)))\n",
        "\n",
        "print(str(ngrams)+'-LSTM model')\n",
        "torch.save(model_onegram.state_dict(),str(ngrams)+'-gram_model.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 18440\n",
            "\n",
            "Epoch: 0/5 Loss: 9.702847803632418\n",
            "Time Taken to run epoch--- 2.55 seconds ---\n",
            "\n",
            "Epoch: 1/5 Loss: 8.973389402031898\n",
            "Time Taken to run epoch--- 2.37 seconds ---\n",
            "\n",
            "Epoch: 2/5 Loss: 7.836528172095616\n",
            "Time Taken to run epoch--- 2.37 seconds ---\n",
            "\n",
            "Epoch: 3/5 Loss: 7.474601216614246\n",
            "Time Taken to run epoch--- 2.36 seconds ---\n",
            "\n",
            "Epoch: 4/5 Loss: 7.306475644310315\n",
            "Time Taken to run epoch--- 2.35 seconds ---\n",
            "1-LSTM model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESIaOO5CO-ih"
      },
      "source": [
        "###2-gram LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-shWAzzO22h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d97e6e9-b372-4095-a28b-282b677ade8a"
      },
      "source": [
        "seq_size=20\n",
        "batch_size=64\n",
        "embedding_size=64\n",
        "lstm_size=64\n",
        "initial_words='my favourite movie'.split()\n",
        "predict_top_k=5\n",
        "checkpoint_path='checkpoint'\n",
        "ngrams=2\n",
        "learning_rate = 0.1\n",
        "\n",
        "clean_full_data = [clean_text(rev) for rev in full_data[0:1000]]\n",
        "text = joinStrings(list(flatten(clean_full_data)))\n",
        "corpus_text = generateNGrams(text.split(),ngrams)\n",
        "in_text,out_text,n_vocab, int_to_vocab_bi,vocab_to_int_bi= prep_data(corpus_text) # changed here\n",
        "batches = get_batches(in_text, out_text, batch_size, seq_size)\n",
        "\n",
        "n_epochs = 5\n",
        "sentences = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_bigram = RNNModule(n_vocab, seq_size,embedding_size, lstm_size)\n",
        "model_bigram = model_bigram.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model_bigram.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  start_time = time.time() \n",
        "  loss_values = train(model_bigram, batches, optimizer, criterion)\n",
        "  print('\\nEpoch: {}/{}'.format(e, n_epochs),\n",
        "        'Loss: {}'.format(np.mean(loss_values)))\n",
        "  time_taken = round(time.time() - start_time,2)\n",
        "  print(\"Time Taken to run epoch--- %s seconds ---\" % (round(time_taken,2)))\n",
        "\n",
        "print(str(ngrams)+'-LSTM model')\n",
        "torch.save(model_bigram.state_dict(),str(ngrams)+'-gram_model.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 121522\n",
            "\n",
            "Epoch: 0/5 Loss: 11.713424469033876\n",
            "Time Taken to run epoch--- 15.2 seconds ---\n",
            "\n",
            "Epoch: 1/5 Loss: 11.707641189297041\n",
            "Time Taken to run epoch--- 15.18 seconds ---\n",
            "\n",
            "Epoch: 2/5 Loss: 11.701880817612013\n",
            "Time Taken to run epoch--- 15.16 seconds ---\n",
            "\n",
            "Epoch: 3/5 Loss: 11.696052705248198\n",
            "Time Taken to run epoch--- 15.19 seconds ---\n",
            "\n",
            "Epoch: 4/5 Loss: 11.690062503019968\n",
            "Time Taken to run epoch--- 15.18 seconds ---\n",
            "2-LSTM model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZamBBKwPBXS"
      },
      "source": [
        "###3-gram LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOB4w6sQO3qH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3cd72b-4724-42ee-ca1e-3d84f307b747"
      },
      "source": [
        "seq_size=20\n",
        "batch_size=64\n",
        "embedding_size=64\n",
        "lstm_size=64\n",
        "initial_words='my favourite movie'.split()\n",
        "predict_top_k=5\n",
        "checkpoint_path='checkpoint'\n",
        "ngrams=3\n",
        "learning_rate = 0.1\n",
        "\n",
        "clean_full_data = [clean_text(rev) for rev in full_data[0:1000]]\n",
        "text = joinStrings(list(flatten(clean_full_data)))\n",
        "corpus_text = generateNGrams(text.split(),ngrams)\n",
        "in_text,out_text,n_vocab, int_to_vocab_tri,vocab_to_int_tri= prep_data(corpus_text) # changed here \n",
        "batches = get_batches(in_text, out_text, batch_size, seq_size)\n",
        "\n",
        "n_epochs = 5\n",
        "sentences = []\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_trigram = RNNModule(n_vocab, seq_size,embedding_size, lstm_size)\n",
        "model_trigram = model_trigram.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model_trigram.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  start_time = time.time() \n",
        "  loss_values = train(model_trigram, batches, optimizer, criterion)\n",
        "  print('\\nEpoch: {}/{}'.format(e, n_epochs),\n",
        "        'Loss: {}'.format(np.mean(loss_values)))\n",
        "  time_taken = round(time.time() - start_time,2)\n",
        "  print(\"Time Taken to run epoch--- %s seconds ---\" % (round(time_taken,2)))\n",
        "\n",
        "print(str(ngrams)+'-LSTM model')\n",
        "torch.save(model_trigram.state_dict(),str(ngrams)+'-gram_model.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size 208302\n",
            "\n",
            "Epoch: 0/5 Loss: 12.252341757218042\n",
            "Time Taken to run epoch--- 28.21 seconds ---\n",
            "\n",
            "Epoch: 1/5 Loss: 12.251896624763807\n",
            "Time Taken to run epoch--- 28.22 seconds ---\n",
            "\n",
            "Epoch: 2/5 Loss: 12.251456355055174\n",
            "Time Taken to run epoch--- 28.21 seconds ---\n",
            "\n",
            "Epoch: 3/5 Loss: 12.251020888487497\n",
            "Time Taken to run epoch--- 28.2 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgES5EGKcav8"
      },
      "source": [
        "# generaring for onegram model \n",
        "start_time = time.time()\n",
        "for _ in range(5):\n",
        "  words = 'my favourite movie'.split()\n",
        "  model_onegram.eval()\n",
        "  sh,sc = model_onegram.zero_state(1)\n",
        "  sh = sh.to(device)\n",
        "  sc = sc.to(device)\n",
        "  for w in gram:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, (state_h, state_c) = model_onegram(ix, (sh, sc))\n",
        "\n",
        "  _, top_ix = torch.topk(output[0], k=20)\n",
        "\n",
        "  choices = top_ix.tolist()\n",
        "  choice = np.random.choice(choices[0])\n",
        "  words.append(int_to_vocab[choice])\n",
        "\n",
        "\n",
        "  for _ in range(20):\n",
        "    ix = torch.tensor([[choice]]).to(device)\n",
        "    output, (state_h, state_c) = model_onegram(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=20)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "    words.append(int_to_vocab[choice])\n",
        "\n",
        "  sentence = ' '.join(words)\n",
        "\n",
        "  sentence_20 = sentence.split()[0:20]\n",
        "\n",
        "  print(' '.join(sentence_20))\n",
        "\n",
        "print('time taken '+str(time.time()-start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_FO6b1e6fXX"
      },
      "source": [
        "\n",
        " # the training data does not have 'my favourite movie' in it so, creating a bigger corpus here to get the random sentences. \n",
        "\n",
        "clean_full_data = [clean_text(rev) for rev in full_data]\n",
        "text = joinStrings(list(flatten(clean_full_data)))\n",
        "corpus_text = generateNGrams(text.split(),ngrams)\n",
        "in_text,out_text,n_vocab, int_to_vocab_tri,vocab_to_int_tri= prep_data(corpus_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCycsgiIxCBM"
      },
      "source": [
        " \n",
        " # generating tri grams \n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        " \n",
        " for _ in range(5): \n",
        "  gram = generateNGrams('my favourite movie'.split(),3)\n",
        "  model_trigram.eval()\n",
        "  sh,sc = model_trigram.zero_state(1)\n",
        "  sh = sh.to(device)\n",
        "  sc = sc.to(device)\n",
        "  for w in gram:\n",
        "    ix = torch.tensor([[vocab_to_int_tri[w]]]).to(device)\n",
        "    output, (state_h, state_c) = model_trigram(ix, (sh, sc))\n",
        "    \n",
        "  _, top_ix = torch.topk(output[0], k=20)\n",
        "\n",
        "  choices = top_ix.tolist()\n",
        "\n",
        "  choice = np.random.choice(choices[0])\n",
        "  gram.append(int_to_vocab_tri[choice])\n",
        "\n",
        "  for _ in range(6):\n",
        "    ix = torch.tensor([[choice]]).to(device)\n",
        "    output, (state_h, state_c) = model_trigram(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=20)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    gram.append(int_to_vocab_tri[choice])\n",
        "\n",
        "  \n",
        "  sentence = ' '.join(gram)\n",
        "\n",
        "  sentence_20 = sentence.split()[0:20]\n",
        "\n",
        "  print(' '.join(sentence_20))\n",
        "\n",
        "print('time take ' + str(time.time()-start_time) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwijNtidgF6i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "82b2bdb0-e04d-4888-8c05-5634359ae89c"
      },
      "source": [
        "# generating for bigram \n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        " \n",
        "for _ in range(5):\n",
        "  input_sentence = 'my favourite movie'.split()\n",
        "  sentence_generated =[]\n",
        "  gram = generateNGrams('my favourite movie'.split(),2)\n",
        "  model_bigram.eval()\n",
        "  sh,sc = model_bigram.zero_state(1)\n",
        "  sh = sh.to(device)\n",
        "  sc = sc.to(device)\n",
        "  for w in gram:\n",
        "    ix = torch.tensor([[vocab_to_int_bi[w]]]).to(device)\n",
        "    output, (state_h, state_c) = model_bigram(ix, (sh, sc))\n",
        "    \n",
        "  _, top_ix = torch.topk(output[0], k=20)\n",
        "\n",
        "  choices = top_ix.tolist()\n",
        "\n",
        "  choice = np.random.choice(choices[0])\n",
        "  sentence_generated.append(int_to_vocab_bi[choice])\n",
        "\n",
        "  for _ in range(8):\n",
        "    ix = torch.tensor([[choice]]).to(device)\n",
        "    output, (state_h, state_c) = model_bigram(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=20)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    sentence_generated.append(int_to_vocab_bi[choice])\n",
        "\n",
        "  sentence = ' '.join(sentence_generated)\n",
        "\n",
        "  sentence_20 = input_sentence + sentence.split()[0:20]\n",
        "\n",
        "\n",
        "  print(' '.join(sentence_20))\n",
        "\n",
        "print('time take ' + str(time.time()-start_time) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my favourite movie focuses only theatre then technology is by selling for the we now hanger chapter explain it small nevada\n",
            "my favourite movie in the college days dolph lundgren given the critics he it is its short inferior sequels been okay\n",
            "my favourite movie career which center of watch list as told there for k a it is basking in of this\n",
            "my favourite movie focuses only hear about in never this movie suits him more vocal hobart as the turkish daniel jackson\n",
            "my favourite movie outraged at season of see this melodrama we to the did not limitations he not with leaving matt\n",
            "time take 0.07631468772888184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FG1wLg0AwFK"
      },
      "source": [
        "# Reference : \n",
        "\n",
        "\n",
        "\n",
        "1.   https://github.com/ChunML/NLP/tree/master/text_generation\n",
        "2.   https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "3.   \n",
        "\n"
      ]
    }
  ]
}